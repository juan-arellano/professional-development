{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability \n",
    "## Part 2\n",
    "### Section 9\n",
    "\n",
    "Probability is the likelihood of an event occuring, an event is a specific outcome or a combination of several outcomes. These outcomes can be pretty much anything. \n",
    "\n",
    "Take flipping a coin for example, since there are two possible outcomes, we have two possible events and two possible probabilities. We express probabilites numerically. Conventionally we write them out using numbers like 0 or 1, or 0.2.\n",
    "\n",
    "Having probability of 1 means absolute certainty of the event occuring, and having a probability of 0 means absolute certainty of the event not occuring. \n",
    "\n",
    "You can think of probability as a field about quantifying how likely an event is. \n",
    "\n",
    "A = event\n",
    "P(A) = probability\n",
    "$P(A) = \\frac{\\text{preferred outcomes}}{\\text{all outcomes}}$\n",
    "\n",
    "Say our preferred outcome of flipping a coin is getting heads, \n",
    "$P(A) = \\frac{1}{2}$\n",
    "\n",
    "Say our preffered outcome is rolling a 4 on a six-sided die, \n",
    "$P(A) = \\frac{1}{6}$\n",
    "\n",
    "Say our preferred outcome is rolling a number divisible by 3 on a six sided die,\n",
    "$P(A) = \\frac{3}{6}$\n",
    "\n",
    "Two independent events happening together can be defined by this\n",
    "$P(A \\text{ and } B) = P(A) * P(B)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected values represent what we expect the outcome to be when we run an experiment many times. Imagine we don't know the probability of getting heads when flipping a coin, one flip is one trial, by completing multiple trials we are conducting an experiment. There's a difference between experimental probabilities and thoeretical probabilities. \n",
    "\n",
    "Experimental probabilities\n",
    "$P(A) = \\frac{\\text{succesful trials}}{\\text{total trials}}$\n",
    "\n",
    "Expected values: the outcome we expect to occur when we run an experiment. We can use expected values to make predictions about the future based on past data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A probability frequency distribution is a collection of the probabilities for each possible outcome. Usually it is expressed with a graph or a table. A complement of an event is everything the event is not. Imagine you are tossing a coin, when it falls we are guaranteed to get either heads or tails. \n",
    "\n",
    "All events have complements and we denote them by adding an apostrophe. The complement of a complement is the actual event. We can subtract the probabilites of a complement from 1 to get the probability of the original event."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 10\n",
    "Permutations represents the number of different possible ways we can arrange a set of elements. Imagine you haven't watched the formula 1 race, but your friends tell you Lewis, Max and Kimi finished in the top three. If Lewis finished 1st, there are two possible outcomes Kimi and Max finished in, one in second and one in third and vice versa. In total this leaves us with 6 different ways these 3 drivers can split the medals. \n",
    "\n",
    "Intuition behind computing total number of permutations for a set of n many elements. We start filling out the position, for instance in the race, we have n possible winners, and 3 top 3 winners. \n",
    "Number of permutations = P_n\n",
    "$P_n = n * (n - 1) * (n - 2) * .. * 1$\n",
    "\n",
    "The notion $n!$ factiorial is the product of the natural numbers from 1 to n. For instance, $3! = 1 * 2 * 3 = 6$. Negative numbers don't have factorials, and $0! = 1$. \n",
    "\n",
    "$n! = (n - 1)! * n$\n",
    "$(n+1)! = n! * (n+1)$\n",
    "\n",
    "$(n+k)! = n! * (n+1) * (n+2) * ... * (n + k)$\n",
    "$(n-k)! = \\frac{n!}{(n-k+1) * (n-k+2) * ... * n}$\n",
    "\n",
    "If we have two natural numbers where $n > k$, then $\\frac{n!}{k!} = (k+1) * (k+2) * ... * n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variations express the total number of ways we can pick and arrage some elements of a givens set. \n",
    "\n",
    "$\\bar{V}^n_p = n^p$\n",
    "\n",
    "where $n$ is the total number of elements we have available and $p$ is the number of positions we need to fill. The number of variations with repetition when picking p-many elements out of n elements, is equal to n to the power of p. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating variations without repetition when arranging p elements out of a total of n is equal to \n",
    "$V_p^n = \\frac{n!}{(n-p)!}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combinations represent the number of different ways we can pick certain elements from a set. Variations don't take into account double counting elements. All the different permutations of a single combination are different variations. \n",
    "\n",
    "Formula:\n",
    "$C_p^n = \\frac{V_p^n}{P_p} = \\frac{n!}{p!(n - p)!}$\n",
    "\n",
    "What if we had to choose 4 out of 10 people to go to a conference\n",
    "$ \\frac{10!}{4! * 6!} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Symmetry of Combinations: We can pick p-many elements in as many ways as we can pick n minus p many elements. Recall where we had to choose 3 out of 10 employees to send to a conference. This is $C_3^10 = 120$, now if we had to choose 7 to go instead of 3, the combination is $C_7^10 = 120$. Thus, we also have 120 different ways of picking the 7 employees. This is the same because picking 7 out of 10 employees to take to the conference is the same as choosing 3 out of 10 to leave behind. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new type of combinations: a combination can be a mixture of different smaller individual events. Imagine the following scenario: the diner in your work consists of a 1 drink, a sandwich, and a side. Each menu consists of 3 types of sandwiches, 2 sides, and 2 drinks. Think about the different parts of the menu as separate positions. For any combination of sandwich and side, we have two ways of completing the menu. 3 * 2 * 2 = 12. Calculating the total number of combinations is by multiplying the number of options available for each individual event."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combinatorics in the lottery: \n",
    "Pick 5 number between 1 - 69\n",
    "\"Powerball\" number betwwen 1 and 26\n",
    "The likelihood of two independent events occuring simulatenously equals the product of their individual probabilities.\n",
    "P(Powerball) = $\\frac{1}{26}$\n",
    "P(5 numbers) = $\\frac{69!}{5! * 64!}$\n",
    "P(lottery) $\\frac{1}{300,000,000}$\n",
    "\n",
    "Two events: getting the correct 5 numbers, and guessing the right powerball number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recap\n",
    "We use permutations with variations when we must arrange a set of object and the order is crucial. In combinations, order is irrelevant. There are two types of variations and combinations, with and without repetitions. $C = \\frac{V}{P}$. We count all the permutations of a given set of numbers as a single combination, but as separate variations.\n",
    "\n",
    "Formulas:\n",
    "- $P_n = n!$\n",
    "- $V_p^n = \\frac{n!}{(n-p)!}$\n",
    "- $C_p^n = \\frac{V_p^n}{P_p} = \\frac{n!}{p!(n - p)!}$\n",
    "\n",
    "\n",
    "With repetition:\n",
    "- $\\bar{V}_p^n = n^p$\n",
    "- $\\bar{C}_p^n = \\frac{(n + p - 1)!)}{p!(n-1)!}$\n",
    "\n",
    "Combinations are symmetric:\n",
    "$C_p^n = C^n_{n-p}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 11\n",
    "We are going to learn how to describe events and how they interact with one another. Every event has a set of outcomes (favourable outcomes). An event can be being a member of the European Union, Values of this set would be France and Germany. Uppercase letters represent sets, and elements are in lowercase. Like set X and element x. Any set can be either empty or have null values, and this makes it the empty set. If an element is part of a set, we use notation $x \\in A$. In other situations, we might need to make generalized statements about multiple elements, we would say \"for all x in A\". The for all sign is $\\forall$, such as \"$\\forall x \\in A : x \\text{ is even}$\". A set that is fully contained in another set is a subset. This notation is: $A \\subset B$. Every set contains at least 2 subsets, the empty set and itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take Events A and B. If the two events never touch, the two events can never happen simulataneously. Event A occuring guarantees that event B is not occuring and vice versa. This would be like getting a diamond or getting a heart, if we get one than we didn't get the other. If they intersect, they can happen. Imagine if Event A is getting a diamond, while event B is getting a queen, then the intersection is the Queen of Diamonds. Imagine if Event B is within Event A, then Event B can only ever occur if the other one does as well. Event A could be drawing a red card, and Event B could be drawing a diamond. If event A does not occur than Event B can happen, however Event A can occur while Event B doesn't. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we want both Event A and Event B to happen at the same time, we are talking about their intersection. This is noted by $A \\cap B$. Ther intersection of all hearts and all diamnonds is the empty set $A \\cap B = \\emptyset$. The intersection of all diamonds and all queens is the queen of diamonds. If Event B happens when Event A happen, then this is noted as $A \\cap B = B$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The union of two sets is the combination of all outcomes preferred for either A or B. The union of two sets is noted as $A \\cup B$. If the sets A and B do not touch at all, $A \\cup B = A + B$. If Sets intersect, then $A \\cup B = A + B - (A \\cap B)$. If B is a subset of A, then $A \\cup B = A$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutually exclusive sets are not allowed to have any overlapping elements. Mutually exclusive sets have the empty set as their intersection. If A and B are mutually exclusive, then $A \\cup B = A + B$. Complements sets are all values that are part of the sample space, but not part of the set. Complements are always mutually exclusive, but not all mutually exclusive sets are complements. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notation: suppose we have two events A and B, the probability of Getting A, if we are given B has occured is noted as $P(A | B)$, or \"A given B\". For instance say, event A is drawing the queen of hearts, and event B is drawing a spade, therefore $P(A | B)$ is the probability of drawing the Queen of Spades if we know the card is a spade. This is called conditional probability and we use it to distinguish dependent and independent events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional probability is the likelihood of an event occuring, assuming a different one has already happened. Let's say Event A is flipping heads, and Event B is flipping heads on the previous flip. $P(A | B) = 0.5$, therefore $P(A) = P(A | B )$, or they are independent. If any two events are independent, $P(A \\cap B) = P(A) * P(B)$. Now consider Event A is drawing the Queen of Spades, Event B is drawing a Spade, and Event C is drawing a Queen. then, $P(A) = \\frac{1}{52}$, and $P(A | B) = \\frac{1}{13}$ while $P(A | C) = \\frac{1}{4}$. A and B are then dependent and A and C are dependent. Then, \n",
    "\n",
    "$$P(A | B) = \\frac{P(A \\cap B)}{P(B)} \\text{ if } P(B) > 0$$\n",
    "\n",
    "The conditional probability formula is very similar to the \"favourable over all\" formula. The order in which we write the elements is crucial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many scientific papers rely on conducting experiments or surveys. Imagine we have a vegetarian survey 100 men and women if they eat meat. 15 women of 47 say they don't eat meat, while 32 of 47 say they do. 15 women and 29 men don't eat meat out of a total of 44. If A represents being vegetarian and B represents being a woman, then P(A | B) does not equal P ( B | A), the first represents the likelihood of a woman being vegetarian ($\\frac{15}{47}$) while the second represents the likelihood of a vegetarian being a woman($\\frac{15}{44}$). It is more likely for a vegetarian to be female, than for a woman NOT to eat meat. The law of total probability is as follows, given $A = B_1 \\cup B_2 \\cup ... \\cup B_n$, then $P(A) = P(A | B_1) * P(B_1) + P (A | B_2) * P(B_2) ...$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The additive law states that the probability of the union two sets is equal to the sum of the individual probabilities of each event, minus the probability of their intersection\n",
    "$$P(A \\cup B) = P(A) + P(B) - P(A \\cap B)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multiplication law states that the probability of $P(A | B) * P(B) = P( A \\cap B)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important formulas in probability is Bayes' Theorem. Given two events, A and B, then $P(A | B) = \\frac{P(B|A) * P(A)}{P(B)}$. In medical research, when trying to find a casual relationship between symptoms, Bayes Theorem is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 12\n",
    "A distribution shows the possible values a variable can take and how frequently they occur. Y = the actual outcome of an event, while y = one of the possible outcomes. P(Y = y) or p(y) mean teh same thing. Y could be the number of red marbles we draw out of a bag, while y = 5 then, P(Y = 5). p(y) is the probability function. Probabilities measure the likelihood of an outcome based one how many time they appear in the sample space. \n",
    "\n",
    "Key definitions:\n",
    "Distributions have two characteristics, mean - average value, and variance - how spread out the data is. The mean of a distribution is noted as $\\mu$ and variance as $\\sigma^2$.\n",
    "\n",
    "Population data is how we refer to \"all of the data\", while sample data is only part of it. Different notations: \n",
    "sample mean: $\\bar{x}$\n",
    "sample variance: $s^2$\n",
    "\n",
    "Standard deviation is the positive square root of variance, or $\\sigma$ when dealing with a population or $s$ when dealing with a sample. A constant relationship exists between mean and variance. \n",
    "\n",
    "$$\\sigma^2 = E((Y - \\mu)^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various types of probability distributions. We will examine the characteristics of the most common distributions. Notation: X, variable, and ~, Tilde, N, type, and ($\\mu, \\sigma^2$), characteristics. \n",
    "\n",
    "Discrete distributions:\n",
    "- Uniform distributions: all outcomes are equally possible\n",
    "- Bernoulli Distribution: events with only two possible outcomes\n",
    "- Binomial Distribution: two outcomes per iteration\n",
    "- Poisson Distribution: test how unusual an event frequency is\n",
    "\n",
    "Continuous distributions:\n",
    "A curve as opposed to bars\n",
    "- Normal distribution: \n",
    "- Student's T distribution: small sample approximation of normal distribution, and accomodates extreme values significantly better\n",
    "- Chi-Squared Distribution: asymmetric, only non-negative values, starts from 0, typically skewed to the right, does not often mirror real life events\n",
    "- Exponential distribution: events that are rapidly changing early on\n",
    "- Logistic distibution: useful in forecast analysis|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discrete distributions\n",
    "- Finitely many distinct outcomes\n",
    "- Every unique outcome must have a probability assigned to it\n",
    "- One peculiarity, $P(Y <= y) = P[Y<(+1)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uniform distribution: \n",
    "- We use the letter U(a,b) where a,b are the end points\n",
    "- All outcomes must have equal probability \n",
    "- One drawback: the expected value provides us no relevant information\n",
    "- We can still calculate mean and variance, however they are completely uninterpretable and posess on predictive power "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bernoulli distribution: \n",
    "- We use Bern(p) to note our Bernoulli distribution, where p is the probability of success\n",
    "- Events with 1 trial and 2 possible outcomes follow this distribution\n",
    "- Usually have to assign which outcome is 0 and which one is 1\n",
    "- Expected value will either be p or 1-p\n",
    "- Conventiolly we usually assign 1 with the probability equal to p\n",
    "- Variance: p(1 - p), regardless of what the expected value is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binomial distribution:\n",
    "- A sequence of identical Bernoulli events\n",
    "- Notation: B(n, p) where n is the number of trials and p is the probability of success of each individual trial\n",
    "- A quiz with 10 T/F questions, guessing on one question is a Bernoulli event, the whole quiz is a Binomial event\n",
    "- If we wish to find out the number of ways in which 4 out of 6 trials can be succesful, we would need to find the number of combinations\n",
    "- Expected value: the sum of all values in the sample space multiplied by their probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poisson distribution:\n",
    "- Notation: Po($\\lambda$) \n",
    "- deals with the frequency with which an event occurs\n",
    "- Instead of a probability it requires how often an event occurs\n",
    "- For instance, a firefly will light up 3 times in 10 seconds, we can use a Poisson distribution to see how many times it lights up in 20 seconds\n",
    "- No event can happen a negative amount of times so the graph starts at 0\n",
    "- Poisson distribution is wildly different, it's probability function is just as different $P(Y) = \\frac{\\lambda^y * e^y}{y!}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuous distributions\n",
    "- Sample space is infinte, therefore we cannot record the frequency of each distinct value\n",
    "- The probability for any individual value is equal to 0\n",
    "- Therefore, P(X < 6) = P(X <= 6)\n",
    "- The relationship between the PDF and the CDF of a continuous distribution is that the CDF is the area under the curve of the PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal distribution:\n",
    "- We define a normal distrubtion using $N(\\mu, \\sigma^2)$\n",
    "- Often appears in nature and life, the size of a full grown male lion follows a normal distribution\n",
    "\n",
    "Distinct characteristics: \n",
    "- Graph follows a bell shape\n",
    "- Graph is symmetric\n",
    "- A lion is equally likely to weight 350 pounds or 450 pounds since they are equally away from the mean\n",
    "- E(X) = $\\mu$\n",
    "- We can deduce variance from the expected value\n",
    "- 68, 95, 99.7, 68% of all outcomes fall 68% within one standard deviaation from the mean, 95% fall within two standard deviations, and 99.7 within 3\n",
    "- Outliers are extremely rare in normal distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A transformation is a way in which we can alter every element of a distribution to get a new distribution, such as addition, subtraction, multiplication, or division. Standardizing is a special kind of transformation in which we make the expected value 0 and the variance 1. The distribution we get when standardizing is teh Standard Normal Distribution. Standardizing is incredibly useful when we have a normal distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student's t distribution\n",
    "- Notation t(k) where k is the degrees of freedom\n",
    "- We use the student's t distribution when we don't have enough a sample size \n",
    "- Curve is also symmetric and bell shaped\n",
    "- The tales are bigger to accomodate the occurence of extreme values away from the mean\n",
    "- If k > 2, the expected value of a t distribution is the mean\n",
    "- Frequently used when conducting statistical analysis\n",
    "- Useful for hypothesis testing with limited data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chi Squared Distribution\n",
    "- We denote it with $\\chi ^2(k)$ where k is the degrees of freedom\n",
    "- Very few events follow such a distribution\n",
    "- Useful for hypothesis testing and computing confidence intervals\n",
    "- Distibution is asymmetric and highly skewed to the right and graph starts at 0 rather than a negative number\n",
    "- Contains a table of known values like the normal and student's t distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exponential distribution\n",
    "- Defined by Exp($\\lambda$) \n",
    "- Start off highs, intiially decreases, then eventually plateauing\n",
    "- Think of a Youtube vlog where it increases in views at the start but eventually plateaus\n",
    "- The curve resembles a sort of boomerang\n",
    "- $\\lambda$ is our rate parameters and is how fast our curve plateaus\n",
    "- The expected value is $\\frac{1}{\\lambda}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic distribution\n",
    "- Defined by Logistic($\\mu, S$), where $\\mu$ is the location and $S$ is the scale\n",
    "- The graph of the Logistic distribution resembles that of the Normal distribution\n",
    "- Used to help us forecast binary outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 13\n",
    "Probability in finance\n",
    "An option is an agreement between two parties for the price of a stock or item at a future point in time. Since this pact puts one of the parties at a disadvantage, the person at an advantage pays for an option. For instance, say you are offered to buy 10 stocks of Google for 1,100 in an option, of which you are paying a 100 premium.The stock price has a .40 likelihood of increasing to 1,200 while there's a .60 chance of it dropping to 1,000. We can make a decistion tree. If the price drops to 1,000 and you do not call the option, you lose 100 and if you do call the option you lose 1,100 dollars. If the price increases and you do not call the option, you still lose 100 but if you call the option you make 900. We can calculate the expected payoff. If it's less than 0, it's disadvantagoues to make the deal. If it's 0, it's a fair deal. If the payoff is positive, then the deal is favourable. The expected payoff can be calculated by the following. Assuming you buy the option, if the price increases, you have a payoff of 900, while if it decreases you lose 100. Expected value = 0.6(-100) + 0.4(900) = 300. This means the deal is favourable. Investors can charge a higher premium to make it a fair deal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabiliy in statistics: statistics focuses predominantly on samples and incomplete data. We then must focus highly on expected values and prediction intervals. Statistics builds on probability, and data science builds on statistics. Imagine you are on a trip to see your favorite soccer team. You want to decide how much money you should bring to eat. You use Yelp to get data on average price of food at 10 restaurants. Using standard deviation, we can calculate our Confidence Interval and it will tell us we should carry between 20 and 25 dollars as that's 95 percent confidence interval of prices of food. The three crucial requirements for hypothesis testing in statistics are mean, variance, and type of distribution. In the field of statistics, we are often provided sample data without knowing the type. When determining the distribution, any distribution predicts a value for all points within our dataset. The distribution anticipate the actual data point. The more distributions we know the easier it will be to determine which one we are dealing with for any problem. What statisticians call as mathematical modeling, data scienctists call supervised machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In data analysis, we usually try to analyze past data, find insights, to then make predictions about the future. In mathematical modeling, we run articificial simulations to see how well our preditctions match up to various future scenarios. In mathematic modeling, we often can run a Monte Carlo simulation, where we generate artificial data to test the predictive power of our mathematical models, we don't use completely random data, but we must follow certain restrictions. In essence, data science is the expansion of probability, statistics, and programming that implements computational technology to solve more advanced questions. Even when using it to predict future outcomes, there is still great uncertainty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

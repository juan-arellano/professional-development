{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal component analysis\n",
    "Machine learning in general works wonders when the dataset provided for training the machine is large and concise. Usually having a good amount of data lets us build a better predictive model since we have more data to train the machine with. However, using a large dataset has its own pitfalls. The biggest pitfall is the curse of dimensionality. \n",
    "\n",
    "It turns out that in large dimenstional datasets, there might be lots of inconsistencies in the features or lots of redundant featurs in the dataset, which only will increase the computation time and make data processing and EDA more convoluted. \n",
    "\n",
    "To get rid of the curse of dimensionality, a process called dimensionality reduction was introduced. Dimensionality reduction techniques can be used to filter only in a limited number of significant features needed for training and this is where PCA comes in. \n",
    "\n",
    "## What is PCA?\n",
    "Principal Components Analysis (PCA) is dimensionality reduction technique that enables you to identify correlations and patters in a data set so that it can be transofmred into a data set of significantly lower dimension without loss of any important information. \n",
    "\n",
    "## Step-by-Step Computation of PCA\n",
    "The below steps need to be followed to perform dimensionality reduction using PCA:\n",
    "\n",
    "- Standardization of the data\n",
    "- Computing the covariance matrix\n",
    "- Calculating the eigenvectors and eigenvalues\n",
    "- Computing the Principal Components\n",
    "- Reducing the dimensions of the data set\n",
    "\n",
    "Here are all the steps in detail: \n",
    "\n",
    "### Step 1: Standardization of the data \n",
    "Standardization is all about scaling your data in such a way that all the variables and their values lie within a similar range. \n",
    "\n",
    "Consider an example, let's say that we have 2 variables in a our data set, one has values ranging between 0 - 100 and the other has values between 1,000 - 5,000. In such a scenario, it is obvious that the output calculated by using these predictor variables is going to be biased since the variable with a larger range will have a more obvious impact on the outcome. \n",
    "\n",
    "Therefore, standardizing the data into a comparable range is very important. Standardization is carried out by subtracting each value in the data from the mean and dividing it by the overall deviation in the data set. \n",
    "\n",
    "It can be calculated like so:\n",
    "\n",
    "Z = (Variable value - mean)/Standard deviation\n",
    "                \n",
    "Post this step, all the variables in the data are scaled across a standard and comparable scale. \n",
    "\n",
    "### Step 2: Computing the covariance matrix\n",
    "As mentioned earlier, PCA helps to identify the correlation and dependencies among the features in a data set. A covariance matrix expresses the correlation between the different variables in the data set. It is essential to identify heavily dependent variables because they contain biased and redundant information which reduces the overall performance of the model. \n",
    "\n",
    "Mathematically, a covariance matrix is a p x p matrix, where p represents the dimensions of the data set. Each entry in the matrix represents the covariance of the corresponding variables. \n",
    "\n",
    "Consider a case where we have a 2D data set with variables a and b, the covariance matrix is a 2 x 2 matrix as show below\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "Cov(a,a) & Cov(a,b) \\\\\n",
    "Cov(b,a) & Cov(b,b) \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "In the above matrix:\n",
    "- Cov(a,a) represents the covariance of a variable with itself, which is nothing but the variance of the variable 'a'\n",
    "- Cov(a,b) represents the covariance of the variable 'a' with respect to the variable 'b'. And since covariance is commutative, Cov(a,b) = Cov(b,a)\n",
    "\n",
    "Here are the key takeaways from the covariance matrix:\n",
    "\n",
    "- The covariance values denotes how co-dependent two variables are with respect to each other\n",
    "- If the covariance value is negative, it denotes the respective variables are indirectly proportional to each other\n",
    "- A positive covariance denotes that the respective variables are directly proportional to each other\n",
    "\n",
    "### Step 3: Calculating the Eigenvectors and Eigenvalues\n",
    "Eigenvectors and eigenvalues are the mathematical constructs that must be computed from the covariance matrix in order to deteremine the principal components of the data set. But first, let's understand more about principal components\n",
    "\n",
    "### What are principal components?\n",
    "Simply put, principal components are the new set of variables that are obtained from the initial set of variables. The principal components are computed in such a manner that newly oobtained variables are highly significant and independent of each other. The principal components compress and possess most of the useful information that was scattered among the initial variables. \n",
    "\n",
    "If your data set is of 5 dimensions, then 5 principal components are computed, such that, the first principal component stores the maximium possible information and the second one stores the remaining maximum info and so on, you get the idea. \n",
    "\n",
    "Now, where do Eigenvectors fall into this whole process?\n",
    "\n",
    "Assuming that all have a basic understanding of Eigenvectors and eigenvalues, we know that these two algebraic forumaltions are always computed as a pair, i.e., for every eigenvector there is an eigenvalue. The dimensions in the data determine the number of eigenvectors that you need to calculate. \n",
    "\n",
    "Consider a 2-Dimensional data set, for which 2 eigenvectors (and their respective eigenvalues) are computed. The idea behind eigenvectors is to use the Covariance matrix to understnad where in the data there is the most amount of variance. Since more variance in the data denotes more information about the data, eigenvectors are used to identify and compute Principal Components. \n",
    "\n",
    "Eigenvalues, on the other hand, simply denote the scalars of the respective eigenvectors. Therefore, eigenvectors and eigenvalues will compute the Principal Components of the data set.\n",
    "\n",
    "### Step 4: Computing the Principal Components\n",
    "Once we have computed the Eigenvectors and eigenvalues, all we have to do is order them in the descending order, where the eigenvector with the highest eigenvalue is the most significant and thus forms the first principal component. The principal components of lesser significances can thus be removed in order to reduce the dimensions of the data. \n",
    "\n",
    "The final step in computing the Principal Components is to form a matrix known as the feature matrix that contains all the significant data variables that possess maximum information about the data. \n",
    "\n",
    "### Step 5: Reducing the dimensions of the data set\n",
    "The last step in performing PCA is to re-arrange the original data with the final principal components which represent the maximum and the most significant information of the data set. In order to replace the original data axis with the newly formed Principal Components, you simply multiple the transpose of the original data set by the transpose of the obtained feature vector. \n",
    "\n",
    "So that was the theory behind the entire PCA process. It's time to get our hands dirty and perform all these steps by using a real data set. \n",
    "\n",
    "## Principal Component Analysis Using Python\n",
    "In this section, we will be performing PCA by using Python.\n",
    "\n",
    "#### Problem statement: \n",
    "To perform step-by-step Principal Component Analysis in order to reduce the dimension of the data set.\n",
    "\n",
    "#### Data Set Description:\n",
    "Movies rating data set that contains ratings from 700+ users for approximately 9,000 movies (features).\n",
    "\n",
    "#### Logic:\n",
    "Perform PCA. by finding the most significant features in teh data. PCA will be perfroemd by following the steps that were defined above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import *\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import register_cmap\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Import data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load movie names and movie ratings\n",
    "movies = pd.read_csv('~/Documents/GitHub/professional-development/PCA/movies.csv')\n",
    "ratings = pd.read_csv('~/Documents/GitHub/professional-development/PCA/ratings.csv')\n",
    "ratings.drop(['timestamp'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Formatting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId                               title  \\\n",
       "0        1                    Toy Story (1995)   \n",
       "1        2                      Jumanji (1995)   \n",
       "2        3             Grumpier Old Men (1995)   \n",
       "3        4            Waiting to Exhale (1995)   \n",
       "4        5  Father of the Bride Part II (1995)   \n",
       "\n",
       "                                        genres  \n",
       "0  Adventure|Animation|Children|Comedy|Fantasy  \n",
       "1                   Adventure|Children|Fantasy  \n",
       "2                               Comedy|Romance  \n",
       "3                         Comedy|Drama|Romance  \n",
       "4                                       Comedy  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movieId', 'title', 'genres']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating\n",
       "0       1        1     4.0\n",
       "1       1        3     4.0\n",
       "2       1        6     4.0\n",
       "3       1       47     5.0\n",
       "4       1       50     5.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_name(x):\n",
    "    return movies[movies['movieId'] == x].title.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.movieId = ratings.movieId.map(replace_name)\n",
    "M = ratings.pivot_table(index=['userId'], columns=['movieId'], values='rating')\n",
    "m = M.shape\n",
    "df1 = M.replace(np.nan, 0, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std = StandardScaler().fit_transform(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance matrix n[[ 1.00164204 -0.00164473 -0.00232791 ...  0.32582147 -0.00819887\n",
      "  -0.00164473]\n",
      " [-0.00164473  1.00164204  0.70768614 ... -0.00360024 -0.00819887\n",
      "  -0.00164473]\n",
      " [-0.00232791  0.70768614  1.00164204 ... -0.00509569 -0.01160448\n",
      "  -0.00232791]\n",
      " ...\n",
      " [ 0.32582147 -0.00360024 -0.00509569 ...  1.00164204 -0.01794692\n",
      "  -0.00360024]\n",
      " [-0.00819887 -0.00819887 -0.01160448 ... -0.01794692  1.00164204\n",
      "  -0.00819887]\n",
      " [-0.00164473 -0.00164473 -0.00232791 ... -0.00360024 -0.00819887\n",
      "   1.00164204]]\n"
     ]
    }
   ],
   "source": [
    "mean_vec = np.mean(X_std, axis=0)\n",
    "cov_mat = (X_std - mean_vec).T.dot((X_std - mean_vec)) / (X_std.shape[0]-1)\n",
    "print('Covariance matrix n%s' %cov_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating eigenvectors and eigenvalues on covariance matrix\n",
    "cov_mat = np.cov(X_std.T)\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "print('Eigenvectors n%s' %eig_vecs)\n",
    "print('nEigenvalues n%s' %eig_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visually confirm that the list is correctly sorted by decreasing eigenvalues\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "print('Eigenvalues in descending order:')\n",
    "for i in eig_pairs:\n",
    "print(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit_transform(df1)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
